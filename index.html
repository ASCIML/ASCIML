<!DOCTYPE html>
<html>
<head>
	<title>ASCIML</title>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Add any necessary stylesheets or scripts here -->
	<style>
		body {
			font-family: Arial, sans-serif;
			background-color: #f5f5f5;
		}
		.container {
			max-width: 1000px;
			margin: 0 auto;
			padding: 20px;
			background-color: #fff;
			box-shadow: 0px 0px 10px rgba(0,0,0,0.1);
		}
		h1 {
			text-align: center;
			margin-top: 0;
		}
		.results {
			margin-top: 20px;
		}
		.result {
			padding: 10px;
			border: 1px solid #ddd;
			margin-bottom: 10px;
			background-color: #fff;
			box-shadow: 0px 0px 5px rgba(0,0,0,0.1);
		}
		.result h2 {
			margin-top: 0;
		}
		.result p {
			margin: 0;
		}
	</style>
</head>
<body>
	<div class="container">
		<h1>Exploring the potential of interactive Machine Learning for Sound Generation: A preliminary study with sound artists</h1>
		<p>Interactive Machine Learning (IML) is an approach previ- ously explored in music discipline [10, 11, 13, 18].
            However, its adaptation in sound synthesis as an algorithmic method of creation has not been examined. 
            This article presents the prototype ASCIML, an Assistant for Sound Creation with Interactive Machine Learning,
             that allows musicians to use IML to create personalized datasets and generate new sounds. 
             Additionally, a preliminary study is presented which aims to evaluate the potential of ASCIML 
             as a tool for sound synthesis and to gather feedback and suggestions for future improvements. 
             The prototype can be used in Google Colaboratory [2] and is divided into four main stages: 
             Data Design, Training, Evaluation and Audio Creation. 
             Results from the study, which involved 27 musicians with no prior knowledge of Machine Learning (ML), 
             showed that most participants preferred using microphone recording and synthesis to design their dataset 
             and that the Envelopegram visualization was found to be particularly meaningful to understand sound datasets. It was also found that the ma- jority of participants preferred to implement a pre-trained model on their data and relied on hearing the audio recon- struction provided by the interface to evaluate the model performance. Overall, the study demonstrates the poten- tial of ASCIML as a tool for hands-on neural audio sound synthesis and provides valuable insights for future develop- ments in the field.</p>
        <div class="results">
			<!-- Add results dynamically here using server-side scripting language or javascript -->
			<div class="result">
				<h2>Result Title</h2>
				<p>Result description goes here...</p>
				<p><a href="#">Link to full result</a></p>
			</div>
			<div class="result">
				<h2>Result Title</h2>
				<p>Result description goes here...</p>
				<p><a href="#">Link to full result</a></p>
			</div>
			<div class="result">
				<h2>Result Title</h2>
				<p>Result description goes here...</p>
				<p><a href="#">Link to full result</a></p>
			</div>
		</div>
	</div>
</body>
</html> 